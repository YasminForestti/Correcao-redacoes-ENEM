{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.get_clause import get_clause_list\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import os \n",
    "from nltk import pos_tag\n",
    "from nltk import Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m nltk\u001b[38;5;241m.\u001b[39mpos_tag(word_tokenize(\u001b[43msentence\u001b[49m), lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt-br\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sentence' is not defined"
     ]
    }
   ],
   "source": [
    "nltk.pos_tag(word_tokenize(sentence), lang=\"pt-br\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path_to_gs=\"C:\\\\Program Files\\\\gs\\\\gs9.50\\\\bin\"   #install Ghostscript to set the PATH environment variable\n",
    "os.environ['PATH']+=os.pathsep+path_to_gs    # modifying environment variable\n",
    "\n",
    "\n",
    "\n",
    "def clauses(sentence):\n",
    "    # define tree representation of syntactic structure of sentence\n",
    "    def cfg_parse(sentence):\n",
    "        # create CFG(context free grammar) to analyze sentence structure)\n",
    "        sent_tk=nltk.pos_tag(word_tokenize(sentence), lang=\"pt-br\")\n",
    "        # creating grammar with three postags NNP, VBD/VBN or NN for sentences having postags only these one.\n",
    "        for one in sent_tk:\n",
    "            if one[1]=='NNP':\n",
    "                s_NP=\"\\'\"+one[0]+\"\\'\" \n",
    "            if one[1]=='VBD' or one[1]=='VBN':\n",
    "                s_V=\"\\'\"+one[0]+\"\\'\"\n",
    "            if one[1]=='NN':\n",
    "                s_N=\"\\'\"+one[0]+\"\\'\"   \n",
    "            \"\"\"....\n",
    "               Applying conditions for other postags\n",
    "               ''''\n",
    "            \"\"\"\n",
    "        cfg_grammar2=nltk.CFG.fromstring(\"\"\"\n",
    "        S  -> NP VP\n",
    "        VP -> V N\n",
    "        NP -> {}\n",
    "        V -> {}\n",
    "        N -> {}\n",
    "        \"\"\".format(s_NP,s_V,s_N))\n",
    "        parser = nltk.ChartParser(cfg_grammar2)\n",
    "        for tree in parser.parse(word_tokenize(sentence)):\n",
    "            t = Tree.fromstring(str(tree))      # Parse a tree from a string with parentheses.\n",
    "            return t\n",
    "    \n",
    "    tree=cfg_parse(sentence)             # tree is NLP parse tree\n",
    "    subtexts = []\n",
    "    for subtree in tree.subtrees():\n",
    "        if subtree.label()==\"S\" or subtree.label()==\"SBAR\":\n",
    "            #print subtree.leaves()\n",
    "            subtexts.append(' '.join(subtree.leaves()))\n",
    "    #print subtexts\n",
    "    \n",
    "    presubtexts = subtexts[:]       # ADDED IN EDIT for leftover check\n",
    "    \n",
    "    for i in reversed(range(len(subtexts)-1)):\n",
    "        subtexts[i] = subtexts[i][0:subtexts[i].index(subtexts[i+1])]\n",
    "    \n",
    "    for text in subtexts:\n",
    "        print (text)\n",
    "    \n",
    "    # ADDED IN EDIT - Not sure for generalized cases\n",
    "    leftover = presubtexts[0][presubtexts[0].index(presubtexts[1])+len(presubtexts[1]):]\n",
    "    return leftover\n",
    "\n",
    "#sentence='John saw boat'\n",
    "#prin(clauses(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\mwuillau/nltk_data'\n    - 'c:\\\\Users\\\\mwuillau\\\\Documents\\\\UFRJ\\\\Correcao-redacoes-ENEM\\\\env\\\\nltk_data'\n    - 'c:\\\\Users\\\\mwuillau\\\\Documents\\\\UFRJ\\\\Correcao-redacoes-ENEM\\\\env\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\mwuillau\\\\Documents\\\\UFRJ\\\\Correcao-redacoes-ENEM\\\\env\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\mwuillau\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mclauses\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mOs gatos são bonitos e os cachorros são azuis\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 39\u001b[0m, in \u001b[0;36mclauses\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m     36\u001b[0m         t \u001b[38;5;241m=\u001b[39m Tree\u001b[38;5;241m.\u001b[39mfromstring(\u001b[38;5;28mstr\u001b[39m(tree))      \u001b[38;5;66;03m# Parse a tree from a string with parentheses.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\n\u001b[1;32m---> 39\u001b[0m tree\u001b[38;5;241m=\u001b[39m\u001b[43mcfg_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m             \u001b[38;5;66;03m# tree is NLP parse tree\u001b[39;00m\n\u001b[0;32m     40\u001b[0m subtexts \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m subtree \u001b[38;5;129;01min\u001b[39;00m tree\u001b[38;5;241m.\u001b[39msubtrees():\n",
      "Cell \u001b[1;32mIn[9], line 14\u001b[0m, in \u001b[0;36mclauses.<locals>.cfg_parse\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcfg_parse\u001b[39m(sentence):\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# create CFG(context free grammar) to analyze sentence structure)\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m     sent_tk\u001b[38;5;241m=\u001b[39m\u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt-br\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# creating grammar with three postags NNP, VBD/VBN or NN for sentences having postags only these one.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m one \u001b[38;5;129;01min\u001b[39;00m sent_tk:\n",
      "File \u001b[1;32mc:\\Users\\mwuillau\\Documents\\UFRJ\\Correcao-redacoes-ENEM\\env\\lib\\site-packages\\nltk\\tag\\__init__.py:165\u001b[0m, in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset, lang)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpos_tag\u001b[39m(tokens, tagset\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, lang\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39meng\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    141\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[39m    Use NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[39m    tag the given list of tokens.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[39m    :rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 165\u001b[0m     tagger \u001b[39m=\u001b[39m _get_tagger(lang)\n\u001b[0;32m    166\u001b[0m     \u001b[39mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
      "File \u001b[1;32mc:\\Users\\mwuillau\\Documents\\UFRJ\\Correcao-redacoes-ENEM\\env\\lib\\site-packages\\nltk\\tag\\__init__.py:107\u001b[0m, in \u001b[0;36m_get_tagger\u001b[1;34m(lang)\u001b[0m\n\u001b[0;32m    105\u001b[0m     tagger\u001b[39m.\u001b[39mload(ap_russian_model_loc)\n\u001b[0;32m    106\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 107\u001b[0m     tagger \u001b[39m=\u001b[39m PerceptronTagger()\n\u001b[0;32m    108\u001b[0m \u001b[39mreturn\u001b[39;00m tagger\n",
      "File \u001b[1;32mc:\\Users\\mwuillau\\Documents\\UFRJ\\Correcao-redacoes-ENEM\\env\\lib\\site-packages\\nltk\\tag\\perceptron.py:167\u001b[0m, in \u001b[0;36mPerceptronTagger.__init__\u001b[1;34m(self, load)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n\u001b[0;32m    165\u001b[0m \u001b[39mif\u001b[39;00m load:\n\u001b[0;32m    166\u001b[0m     AP_MODEL_LOC \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfile:\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(\n\u001b[1;32m--> 167\u001b[0m         find(\u001b[39m\"\u001b[39;49m\u001b[39mtaggers/averaged_perceptron_tagger/\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m PICKLE)\n\u001b[0;32m    168\u001b[0m     )\n\u001b[0;32m    169\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload(AP_MODEL_LOC)\n",
      "File \u001b[1;32mc:\\Users\\mwuillau\\Documents\\UFRJ\\Correcao-redacoes-ENEM\\env\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\mwuillau/nltk_data'\n    - 'c:\\\\Users\\\\mwuillau\\\\Documents\\\\UFRJ\\\\Correcao-redacoes-ENEM\\\\env\\\\nltk_data'\n    - 'c:\\\\Users\\\\mwuillau\\\\Documents\\\\UFRJ\\\\Correcao-redacoes-ENEM\\\\env\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\mwuillau\\\\Documents\\\\UFRJ\\\\Correcao-redacoes-ENEM\\\\env\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\mwuillau\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "clauses('Os gatos são bonitos e os cachorros são azuis')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
